{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter1 基础知识\n",
    "\n",
    "#### Representation\n",
    "\n",
    "* One-Hot Representation\n",
    "\n",
    "* TF Representation\n",
    "\n",
    "  短语、句子或文档的TF表示仅仅是构成词的one-hot的总和\n",
    "\n",
    "* TF-IDF Representation\n",
    "\n",
    "  有些文档中，重复多次的一些常见词并不能增加我们对文档的理解，而一些罕见词出现的概率比较低，但很可能表明文档的性质，我们希望在我们的表述中赋予它更大的权重。反文档频率（IDF）是一种启发式算法，可以精确地做到这一点。\n",
    "\n",
    "  其中**TF**指的是某词**在文章中**出现的总次数，该指标通常会被归一化定义为TF=（某词在文档中出现的次数/文档的总词量），这样可以防止结果偏向过长的文档（同一个词语在长文档里通常会具有比短文档更高的词频）。**IDF逆向文档频率，包含某词语的文档越少，IDF值越大**，说明该词语具有很强的区分能力，IDF=loge（语料库中文档总数/包含该词的文档数+1），+1的原因是避免分母为0。TFIDF=TFxIDF，TFIDF值越大表示该特征词对这个文本的重要性越大。\n",
    "\n",
    "#### Target Encoding\n",
    "\n",
    "* 目标为文本：one-hot编码\n",
    "* 分类\n",
    "* 预测数值：文章评分、评论预测\n",
    "\n",
    "### PyTorch Basics\n",
    "\n",
    "“tape-based automatic differentiation”可以动态定义和执行计算图形\n",
    "\n",
    "动态 VS 静态 计算图\n",
    "\n",
    ">像Theano、Caffe和TensorFlow这样的静态框架需要首先声明、编译和执行计算图。虽然这会导致非常高效的实现(在生产和移动设置中非常有用)，但在研究和开发过程中可能会变得非常麻烦。像Chainer、DyNet和PyTorch这样的现代框架实现了动态计算图，从而支持更灵活的命令式开发风格，而不需要在每次执行之前编译模型。动态计算图在建模NLP任务时特别有用，每个输入可能导致不同的图结构。\n",
    "\n",
    "PyTorch是一个优化的张量操作库。核心是张量\n",
    "接下来将学习\n",
    "* 创建张量\n",
    "* 操作与张量\n",
    "* 索引、切片和张量连接\n",
    "* 用张量计算梯度\n",
    "* 使用带有gpu的CUDA张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors\n",
    "首先定义一个辅助函数，描述x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"Type:{}\".format(x.type()))\n",
    "    print(\"Shape/size:{}\".format(x.shape))\n",
    "    print(\"Values:\\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定一个**随机张量**的**维数**来初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[9.2737e-41, 5.2908e-38, 5.2939e-38],\n",
      "        [1.9266e-37, 1.7228e-34, 4.8282e-20]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "describe(torch.Tensor(2,3))#指定维数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过随机初始化值区间上的均匀分布（0，1）或者标准正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0.1222, 0.8539, 0.0141],\n",
      "        [0.8869, 0.4828, 0.0055]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[ 0.4222, -3.5212, -0.3114],\n",
      "        [-0.2450, -1.6684,  1.1453]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.rand(2,3))#uniform random\n",
    "describe(torch.randn(2,3))#random normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有张量都用相同的标量填充，0和1有内置函数，特定值可以使用fill_()方法，**任何带有下划线(\\_)的pytorch方法都是指就地操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.zeros(2,3))\n",
    "x=torch.ones(2,3)\n",
    "describe(x)\n",
    "x.fill_(5)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用python列表以声明的方式创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor([[1,2,3],\n",
    "               [4,5,6]])\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值可以来自列表，也可以来自Numpy数组，也可以从张量变换到Numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.DoubleTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0.5336, 0.5114, 0.6819],\n",
      "        [0.3852, 0.4945, 0.4983]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "npy=np.random.rand(2,3)\n",
    "describe(torch.from_numpy(npy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Types and Size\n",
    "使用torch时默认的张量类型时torch.FloatTensor，但是可以初始化时置定，也可以后面进行类型转换\n",
    "两种方法指定初始化类型\n",
    "* 直接调用特定张量类型的构造函数\n",
    "* 使用特殊的方法torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.FloatTensor([[1,2,3],\n",
    "                    [4,5,6]])\n",
    "describe(x)\n",
    "x=x.long()\n",
    "describe(x)\n",
    "x=torch.tensor([[1,2,3],\n",
    "               [4,5,6]],dtype=torch.int64)\n",
    "describe(x)\n",
    "x=x.float()\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Operations\n",
    "可以用+、-、\\*、/进行操作，还可以使用.add()之类的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[ 0.0886,  1.0332,  1.0427],\n",
      "        [-0.0081, -1.4300,  1.1471]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(2,3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[ 0.1773,  2.0665,  2.0855],\n",
      "        [-0.0161, -2.8601,  2.2941]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.add(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[ 0.1773,  2.0665,  2.0855],\n",
      "        [-0.0161, -2.8601,  2.2941]])\n"
     ]
    }
   ],
   "source": [
    "describe(x+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些运算可以应用到特定的为维数上，将行表示为维度0，列表示维度1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([6])\n",
      "Values:\n",
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(6)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x=x.view(2,3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([3])\n",
      "Values:\n",
      "tensor([3, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.sum(x,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values:\n",
      "tensor([ 3, 12])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.sum(x,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values:\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.transpose(x,0,1))#交换维度、转置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing，slicing and joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(6).view(2,3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([1, 2])\n",
      "Values:\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "describe(x[:1,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values:\n",
      "tensor([[0, 2],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "indices=torch.LongTensor([0,2])\n",
    "describe(torch.index_select(x,dim=1,index=indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "indices=torch.LongTensor([0,0])\n",
    "describe(torch.index_select(x,dim=0,index=indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2])\n",
      "Values:\n",
      "tensor([0, 4])\n"
     ]
    }
   ],
   "source": [
    "row_indices=torch.arange(2).long()\n",
    "col_indices=torch.LongTensor([0,1])\n",
    "describe(x[row_indices,col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意指标indices是一个long张量**，这是使用pytorch函数进行索引的要求\n",
    "还可以使用内置的连接函数连接张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([4, 3])\n",
      "Values:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.cat([x,x],dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 6])\n",
      "Values:\n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.cat([x,x],dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2, 3])\n",
      "Values:\n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.stack([x,x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch还在张量上实现了高效的线性代数操作，如乘法、逆、trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.arange(6).view(2,3)\n",
    "describe(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 2])\n",
      "Values:\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x2=torch.ones(3,2)\n",
    "x2[:,1]+=1\n",
    "describe(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'mat2' in call to _th_mm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2e1af11a54a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2' in call to _th_mm"
     ]
    }
   ],
   "source": [
    "describe(torch.mm(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values:\n",
      "tensor([[ 3,  6],\n",
      "        [12, 24]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.mm(x1,x2.long()))#不知道为啥参数二要long类型呢！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors and Computational Graphs\n",
    "requires_grad布尔标志设置为True的张量，记账操作启用，可以追踪梯度的张量以及梯度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(2,2,requires_grad=True)\n",
    "describe(x)\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([2, 2])\n",
      "Values:\n",
      "tensor([[21., 21.],\n",
      "        [21., 21.]], grad_fn=<AddBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "y=(x+2)*(x+5)+3\n",
    "describe(y)\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([])\n",
      "Values:\n",
      "21.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z=y.mean()\n",
    "describe(z)\n",
    "z.backward()\n",
    "print(x.grad is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当您使用requires_grad=True创建张量时，您需要PyTorch来管理计算梯度的bookkeeping信息。首先，PyTorch将跟踪向前传递的值。然后，在计算结束时，使用单个标量来计算向后传递。反向传递是通过对一个张量使用backward()方法来初始化的，这个张量是由一个损失函数的求值得到的。向后传递为参与向前传递的张量对象计算梯度值。\n",
    "\n",
    "一般来说，梯度是一个值，它表示函数输出相对于函数输入的斜率。在计算图形设置中，模型中的每个参数都存在梯度，可以认为是该参数对误差信号的贡献。在PyTorch中，可以使用.grad成员变量访问计算图中节点的梯度。优化器使用.grad变量更新参数的值。\n",
    "\n",
    "到目前为止，我们一直在CPU内存上分配张量。在做线性代数运算时，如果你有一个GPU，那么利用它可能是有意义的。要利用GPU，首先需要分配GPU内存上的张量。对gpu的访问是通过一个名为CUDA的专门API进行的。CUDA API是由NVIDIA创建的，并且仅限于在NVIDIA gpu上使用。PyTorch提供的CUDA张量对象在使用中与常规cpu绑定张量没有区别，除了内部分配的方式不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Tensors\n",
    "首先使用cuda.is_available检查GPU是否可用，然后使用device来检索设备名，实例化所有未来张量，并使用.to(device)方法将其移动到设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因为人家没有GPU，所以下面几行只是示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 3])\n",
      "Values:\n",
      "tensor([[0.4012, 0.1451, 0.7673],\n",
      "        [0.3841, 0.2969, 0.3485],\n",
      "        [0.8788, 0.7676, 0.2608]])\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\"if torch.cuda.is_available()else \"cpu\")\n",
    "print(device)\n",
    "x=torch.rand(3,3).to(device)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要对CUDA和非CUDA对象进行操作，我们需要确保它们在同一设备上。如果我们不这样做，计算就会中断，如下面的代码片段所示。例如，在计算不属于计算图的监视指标时，就会出现这种情况。当操作两个张量对象时，确保它们在同一个设备上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Create a 2D tensor and then add a dimension of size 1 inserted at dimension 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([1, 3, 3])\n",
      "Values:\n",
      "tensor([[[0.4761, 0.3617, 0.0491],\n",
      "         [0.8083, 0.7042, 0.4795],\n",
      "         [0.8808, 0.9778, 0.1992]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,3)\n",
    "describe(a.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the extra dimension you just added to the previous tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 3])\n",
      "Values:\n",
      "tensor([[0.4761, 0.3617, 0.0491],\n",
      "        [0.8083, 0.7042, 0.4795],\n",
      "        [0.8808, 0.9778, 0.1992]])\n"
     ]
    }
   ],
   "source": [
    "describe(a.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random tensor of shape 5x3 in the interval$[3,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([5, 3])\n",
      "Values:\n",
      "tensor([[3.3985, 6.9583, 5.6485],\n",
      "        [3.8364, 6.0906, 4.3738],\n",
      "        [3.2104, 4.4705, 5.9363],\n",
      "        [6.6622, 4.6734, 4.0849],\n",
      "        [3.7328, 5.6520, 5.3473]])\n"
     ]
    }
   ],
   "source": [
    "describe(3+torch.rand(5,3)*(7-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor with values from a normal distribution (mean=0, std=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 3])\n",
      "Values:\n",
      "tensor([[ 1.2487, -0.6895,  1.1554],\n",
      "        [ 0.9867, -0.6859, -1.4196],\n",
      "        [ 0.1196, -0.3800, -0.1261]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,3)\n",
    "a.normal_()\n",
    "describe(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the indexes of all the nonzero elements in the tensor torch.Tensor([1, 1, 1, 0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.LongTensor\n",
      "Shape/size:torch.Size([4, 1])\n",
      "Values:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,1,1,0,1])\n",
    "describe(torch.nonzero(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random tensor of size (3,1) and then horizontally stack 4 copies together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 4])\n",
      "Values:\n",
      "tensor([[0.4735, 0.4735, 0.4735, 0.4735],\n",
      "        [0.8504, 0.8504, 0.8504, 0.8504],\n",
      "        [0.9209, 0.9209, 0.9209, 0.9209]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,1)\n",
    "describe(a.expand(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the batch matrix-matrix product of two 3-dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 4, 4])\n",
      "Values:\n",
      "tensor([[[0.9024, 0.6323, 0.4435, 1.3555],\n",
      "         [1.1262, 0.2698, 1.0911, 1.4384],\n",
      "         [2.0878, 1.0117, 0.9372, 2.0792],\n",
      "         [1.2659, 0.7951, 0.7077, 1.6976]],\n",
      "\n",
      "        [[1.4019, 1.8875, 1.0464, 1.1980],\n",
      "         [0.9163, 0.8831, 0.7357, 1.1266],\n",
      "         [1.0654, 1.4793, 1.2384, 1.3890],\n",
      "         [1.5314, 1.8366, 1.3605, 1.9836]],\n",
      "\n",
      "        [[1.7288, 1.4488, 1.1555, 1.9709],\n",
      "         [1.3053, 1.0602, 0.5968, 1.1216],\n",
      "         [2.0192, 1.2597, 1.1216, 1.9084],\n",
      "         [1.6089, 1.5092, 1.0450, 1.7525]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,4,5)\n",
    "b=torch.rand(3,5,4)\n",
    "describe(torch.bmm(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:torch.FloatTensor\n",
      "Shape/size:torch.Size([3, 4, 4])\n",
      "Values:\n",
      "tensor([[[0.8094, 1.0189, 1.5556, 1.1038],\n",
      "         [0.5987, 0.1652, 0.7446, 0.8852],\n",
      "         [1.0206, 0.4003, 1.2812, 1.4929],\n",
      "         [0.7184, 0.4711, 1.0617, 1.1382]],\n",
      "\n",
      "        [[0.8527, 0.5892, 1.2540, 1.1292],\n",
      "         [0.4010, 0.4652, 0.8599, 1.0789],\n",
      "         [0.5765, 0.8502, 1.3440, 1.0508],\n",
      "         [0.7399, 0.7774, 1.1345, 0.8175]],\n",
      "\n",
      "        [[0.7309, 1.2840, 1.5870, 0.9359],\n",
      "         [0.6918, 0.6223, 1.0887, 1.1120],\n",
      "         [0.7645, 0.5329, 0.9574, 0.8685],\n",
      "         [0.8045, 0.2571, 0.9319, 0.9734]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,4,5)\n",
    "b=torch.rand(5,4)\n",
    "b=b.unsqueeze(0).expand(a.size(0),*b.size())\n",
    "describe(torch.bmm(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 创建张量                             |                                |                                      |                     |\n",
    "| ------------------------------------ | ------------------------------ | ------------------------------------ | ------------------- |\n",
    "| torch.tensor(dim)                    | 指定维数生成                   |                                      |                     |\n",
    "| torch.rand(dim)                      | (0,1)均匀分布                  | torch.randn(dim)                     | 标准正态分布        |\n",
    "| torch.zeros(dim)                     |                                | torch.ones(dim)                      |                     |\n",
    "| tensor.fill_(scalar)                 | 所有张量都用i相同标量填充      | torch.tensor(list)                   | 用列表创建          |\n",
    "| torch.from_numpy(npy)                | 用numpy数组创建                |                                      |                     |\n",
    "| **Types and Size**                   |                                |                                      |                     |\n",
    "| torch.FloatTensor()                  | 特定张量类型的构造函数         | torch.tensor(list,dtype=torch.int64) |                     |\n",
    "| tensor.view(shape)                   | 整形                           | torch.sum(x,dim)                     | 某个维度上求和      |\n",
    "| torch.transpose(x,dim1,dim2)         | 交换维度，转置                 |                                      |                     |\n",
    "| tensor.long()                        | 类型转换                       | torch.LongTensor()                   |                     |\n",
    "| **Indexing,slicing and joining**     |                                |                                      |                     |\n",
    "| tensor[indices]                      | indices是long张量              | python里的切片啥的都可               |                     |\n",
    "| torch.cat([x,x],dim)                 |                                | torch.stack([x,x])                   |                     |\n",
    "| torch.mm(x1,x2)                      | x2好像要long类型的             | torch.bmm(bx1,bx2)                   | 批量矩阵乘          |\n",
    "| **Tensors and Computational Graphs** |                                |                                      |                     |\n",
    "| requires_grad=True                   | 设置为True张量，记账操作启用   | x.grad                               |                     |\n",
    "| z.backward()                         |                                |                                      |                     |\n",
    "| **CUDA**                             |                                |                                      |                     |\n",
    "| torch.cuda.is_available()            | cuda可用？                     | .to(device)                          |                     |\n",
    "|                                      |                                |                                      |                     |\n",
    "| squeeze(0)                           | 在维度0上增加一个维度          | 3+torch.rand(5,3)*(7-3)              | 生成[3，7）均匀分布 |\n",
    "| x.normal_()                          | 归一化（均值0，方差1）         | torch.nonzero()                      | 非0元素的indexes    |\n",
    "| x.expand                             | 以多个自己的copy扩张到指定维度 |                                      |                     |\n",
    "|                                      |                                |                                      |                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
